{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import socket\n",
    "import datetime\n",
    "import itertools\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, BooleanType, DateType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import udf, lit, array_contains, col, when, concat_ws, to_timestamp, from_unixtime, unix_timestamp\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<property at 0x7f1adcb74f18>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparkContext.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "# Configure environment (note: does NOT work with java-11)\n",
    "os.environ[\"JAVA_HOME\"] = \"/java_home\"\n",
    "os.environ['SPARK_HOME'] = '/spark_home'\n",
    "os.environ['HADOOP_CONF_DIR'] = '/hadoop_dir'\n",
    "os.environ['YARN_CONF_DIR'] = '/yarn_dir'\n",
    "os.environ['HADOOP_OPTS'] = '/hadoop_opts'\n",
    "os.environ['_JAVA_OPTIONS'] = '/java_options'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder configs\n",
    "main_folder = 'main_folder_name'\n",
    "time_now = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "debug_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark config\n",
    "config = SparkConf().setAll([('spark.executor.memory', '10g'),\n",
    "                             ('spark.executor.cores', '2'),\n",
    "                             ('spark.yarn.executor.memoryOverhead','5000')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time: 0:00:12.335996\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "sc = pyspark.SparkContext(conf=config)\n",
    "spark = SparkSession.builder.appName('hive').getOrCreate()\n",
    "stop_time = datetime.datetime.now()\n",
    "print(\"Run time: \" + str(stop_time-start_time))\n",
    "# running spark 2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug_flag: sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext\n",
    "\n",
    "sqlContext = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a Impala database that is already located in a Hadoop environment \n",
    "# You could build your own set of files and load them in Spark without Impala\n",
    "sqlContext.sql(\"use db_panjiva\")\n",
    "dates_table = sqlContext.sql('SELECT * FROM panjivausimportdates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- panjivarecordid: long (nullable = true)\n",
      " |-- sourcecountry: string (nullable = true)\n",
      " |-- direction: string (nullable = true)\n",
      " |-- datayear: long (nullable = true)\n",
      " |-- xfcreationdate: timestamp (nullable = true)\n",
      " |-- panjivadataitemid: long (nullable = true)\n",
      " |-- dataitemvalue: timestamp (nullable = true)\n",
      " |-- volumeteu: double (nullable = true)\n",
      " |-- arrivaldate: timestamp (nullable = true)\n",
      " |-- portofunlading: string (nullable = true)\n",
      " |-- concountry: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imports = sqlContext.sql(\"\"\"Select panjivarecordid,\n",
    "volumeteu,\n",
    "arrivaldate,\n",
    "portofunlading,\n",
    "concountry\n",
    "from panjivausimport \n",
    "where concountry = 'United States' OR concountry = 'None'\"\"\")\n",
    "dates_table = dates_table.join(imports, how='inner',on='panjivarecordid')\n",
    "dates_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_table = dates_table.withColumn('load_timestamp', f.unix_timestamp(col('xfcreationdate')))\n",
    "dates_table = dates_table.withColumn('year', f.year(col('arrivaldate')))\n",
    "dates_table = dates_table.withColumn('month', f.month(col('arrivaldate'))+1) #moving everything one month forward\n",
    "dates_table = dates_table.withColumn('day', lit(int(\"01\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_table =(dates_table.withColumn(\"year\", when(f.col(\"month\") > 12, (f.col(\"year\")+1)) #moving month \"13\" forward to the next year\n",
    "                                      .otherwise(f.col(\"year\"))))\n",
    "\n",
    "dates_table = (dates_table.withColumn(\"month\", when(f.col(\"month\") > 12, f.col(\"month\")-12) #changing month \"13 to \"1\"\n",
    "                                      .otherwise(f.col(\"month\"))))\n",
    "#into a date\n",
    "cols=[\"year\",\"month\",\"day\"]\n",
    "dates_table = (dates_table.withColumn(\"data_timestamp\",concat_ws(\"-\",*cols))\n",
    "            .withColumn('data_timestamp', f.unix_timestamp(col('data_timestamp'), \"yyyy-MM-dd\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of interested ports\n",
    "li =['The Port of Los Angeles, Los Angeles, California',\n",
    "     \"Port of Tacoma, Tacoma, Washington\",\n",
    "      \"Port of Seattle, Seattle, Washington\",\n",
    "      'Port of Long Beach, Long Beach, California',\n",
    "      \"New York/Newark Area, Newark, New Jersey\",\n",
    "      \"New York, New York\",\n",
    "      \"Georgia Ports Authority, Savannah, Georgia\",\n",
    "      \"Houston, Houston, Texas\",\n",
    "     \"Port of Virginia, Norfolk, Virginia\",\n",
    "     \"Port of Oakland, Oakland, California\",\n",
    "    \"The Port of Charleston, Charleston, South Carolina\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_table = dates_table.filter(dates_table.portofunlading.isin(li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates_table.select('portofunlading').distinct().sort(col('portofunlading').asc()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      portofunlading|\n",
      "+--------------------+\n",
      "|Georgia Ports Aut...|\n",
      "|Houston, Houston,...|\n",
      "|  New York, New York|\n",
      "|New York/Newark A...|\n",
      "|Port of Long Beac...|\n",
      "|Port of Oakland, ...|\n",
      "|Port of Seattle, ...|\n",
      "|Port of Tacoma, T...|\n",
      "|Port of Virginia,...|\n",
      "|The Port of Charl...|\n",
      "|The Port of Los A...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dates_table.select('portofunlading').distinct().sort(col('portofunlading').asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_table = dates_table.withColumn('date_diff', f.round(col('load_timestamp') - col('data_timestamp'))/(60*60*24))\n",
    "\n",
    "date_cutoff_names = []\n",
    "for i in list(range(-31,31)) + [40,50,60]: #1-30, 40, 50, 60\n",
    "    dates_table = dates_table.withColumn('date_cutoff_' + str(i), col('date_diff') <= i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating folders with time stamp inside for tracking jobs\n",
    "program_name = 'teu_delay'\n",
    "if not os.path.exists(main_folder + '/' + program_name):\n",
    "    os.mkdir(main_folder + '/' + program_name)\n",
    "if not os.path.exists(main_folder + '/' + program_name + '/' + time_now):\n",
    "    current_program = main_folder + '/' + program_name + '/' + time_now\n",
    "    os.mkdir(current_program)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for delay_i in list(range(-31,31)) + [40,50,60] : \n",
    "    col_name = 'date_cutoff_' + str(delay_i) \n",
    "    #print(col_name)\n",
    "    \n",
    "    working_data_agg = (dates_table\n",
    "                        .withColumn('date_cutoff', lit(str(delay_i)))\n",
    "                        .groupBy('year','month','date_cutoff',str(col_name), 'portofunlading')\n",
    "                        .agg(f.round(f.expr('sum(volumeteu)'),3).alias('sum_teu'))\n",
    "                        )\n",
    "\n",
    "    #getting teu total for month\n",
    "    working_data_total = (working_data_agg\n",
    "                            .groupBy('year','month','date_cutoff', 'portofunlading')\n",
    "                            .agg(f.round(f.expr('sum(sum_teu)'),3).alias('sum_teu_total')))\n",
    "    \n",
    "    #getting percent delay per month and joining with true monthly\n",
    "    working_data_percent = (working_data_total\n",
    "                             .join(working_data_agg, how='left',on=['year','month','date_cutoff', 'portofunlading'])\n",
    "                             .filter(col(col_name) == False)\n",
    "                             .withColumn(\"date_delay_percent\",(1-(col(\"sum_teu\")/col('sum_teu_total')))*100) \n",
    "                             .withColumnRenamed(col_name, 'date_cutoff_num'))\n",
    "    \n",
    "    #exporting dataframe into folder\n",
    "    working_data_percent.toPandas().reset_index(drop = True).to_csv(os.path.join(current_program + '/' + program_name + \n",
    "                                                                                   '-pull_' +  str(col_name) +  '.csv')) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
